{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (58.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: torch==1.10.1 in /opt/conda/lib/python3.9/site-packages (from torchtext) (1.10.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from torchtext) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from torchtext) (4.62.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchtext) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.10.1->torchtext) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->torchtext) (3.1)\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "     |████████████████████████████████| 13.9 MB 12.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.2.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.2.0/de_core_news_sm-3.2.0-py3-none-any.whl (19.1 MB)\n",
      "     |████████████████████████████████| 19.1 MB 12.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from de-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (58.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->de-core-news-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "We are using  cuda\n"
     ]
    }
   ],
   "source": [
    "#uncomment this if you are not using puffer\n",
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "#my university container always require reinstalling these dependencies; uncomment this if you don't need to\n",
    "!pip install -U spacy\n",
    "!pip install torchtext\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"We are using \", device)\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "import operator\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - TorchText + biGRU + Attention + Beam Search Decoding + Packed Padded Sequences + Masking + BLEU\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we will be adding one improvement - beam search decoding.  Decoding based on the most probable words may not always be the best.  Allowing the network to try different possible paths may able to yield better sentences.  That is the idea of beam search decoding, i.e., parallely finding good sentences.\n",
    "\n",
    "This is edited from https://github.com/shawnwun/NNDIAL.\n",
    "\n",
    "Everything is almost similar to previous tutorials so I will skip most instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TRG_LANGUAGE = 'en'\n",
    "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\\n',\n",
       " 'Two young, White males are outside near many bushes.\\n')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try print one train sample\n",
    "#a pair of src sentence (de) and target sentence (en)\n",
    "sample = next(train_iter)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  Two young, White males are outside near many bushes.\n",
      "\n",
      "Tokenization:  ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"English sentence: \", sample[1])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2208, 11, 4, 0, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform[TRG_LANGUAGE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "\n",
    "#if we simply use train_iter, once we run it, it cannot be iterate more, so let's convert to map style dataset\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "valid_dataset = to_map_style_dataset(valid_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape:  torch.Size([25, 64])\n",
      "trg shape:  torch.Size([26, 64])\n"
     ]
    }
   ],
   "source": [
    "src, _, trg = next(iter(train_loader))\n",
    "print(\"src shape: \", src.shape) # (seq len, batch_size)\n",
    "print(\"trg shape: \", trg.shape) # (seq len, batch_size)  #if you like batch size first, simply set batch_first = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Seq2Seq Model\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=False)\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)        \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch size, src len, enc hid dim]        \n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2)))\n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #attention = [batch size, src len]\n",
    "\n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        #attention = [batch size, src len]\n",
    "\n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        #simply the SOS_IDX for all batches\n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        #embedded = [batch size, emb dim]\n",
    "        \n",
    "        output = output.squeeze(0)\n",
    "        #output = [batch size, dec hid dim * n directions]\n",
    "        \n",
    "        weighted = weighted.squeeze(0)\n",
    "        #weighted = [batch size, enc hid dim * 2]\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
    "        #prediction = [batch size, output dim]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #a = [batch size, src len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "Here we shall add three methods, the <code>decode</code> method which is simply a factory function that route to greedy decoding or beam search decoding.  The greedy decoding is quite straightforward.  As for the beam search, we are using PriorityQueues which attempts to only retrieve some top nodes with the highest probabilities.  Using PriorityQueues relieve us from implementing many boiler code for deciding which node has the highest probabilities.\n",
    "\n",
    "Another difference is that beam_search_decoding has to be done sentence by sentence, thus the batch size is indexed and reduced to only 1.  To keep the dimension same, we unsqueeze 1 dimension for the batch size.\n",
    "\n",
    "Note that beam width is a crucial factor.  In industry, 10 is often used.   Anyhow, it is important to normalize the score of node by the length (for more details, read https://arxiv.org/abs/1808.10006).   \n",
    "\n",
    "The code otherwise is mostly self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)  #permute so it's the same shape as attention\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is the probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.5 we use teacher forcing 50% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def decode(self, src, src_len, trg, method='beam-search'):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src len = [batch size]\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len) \n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "       \n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        \n",
    "        if method == 'beam-search':\n",
    "            return self.beam_decode(src, trg, hidden, encoder_outputs)\n",
    "        else:\n",
    "            return self.greedy_decode(trg, hidden, encoder_outputs)\n",
    "\n",
    "    def greedy_decode(self, trg, decoder_hidden, encoder_outputs):\n",
    "        seq_len, batch_size = trg.size()\n",
    "        decoded_batch = torch.zeros((batch_size, seq_len))\n",
    "        decoder_input = Variable(trg.data[0, :]).cuda()  # SOS_IDX\n",
    "        for t in range(seq_len):\n",
    "            decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            topi = topi.view(-1)\n",
    "            decoded_batch[:, t] = topi\n",
    "\n",
    "            decoder_input = topi.detach().view(-1)\n",
    "\n",
    "        return decoded_batch\n",
    "\n",
    "    def beam_decode(self, src_tensor, target_tensor, decoder_hiddens, encoder_outputs=None):\n",
    "        #src_tensor = [src len, batch size]\n",
    "        #target_tensor = [trg len, batch size]\n",
    "        #decoder_hiddens = [1, batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        #target_tensor = [batch size, trg len]\n",
    "        \n",
    "        #how many parallel searches\n",
    "        beam_width = 10\n",
    "        \n",
    "        #how many sentence do you want to generate\n",
    "        topk = 1  \n",
    "        \n",
    "        #final generated sentence\n",
    "        decoded_batch = []\n",
    "                \n",
    "        # decoding goes sentence by sentence\n",
    "        for idx in range(target_tensor.size(0)):  # batch_size\n",
    "            \n",
    "            #decoder_hiddens = [1, batch size, dec hid dim]\n",
    "            decoder_hidden = decoder_hiddens[:, idx, :] #.unsqueeze(0)\n",
    "            #decoder_hidden = [1, dec hid dim]  #because we index, that dimension is gone\n",
    "            \n",
    "            #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "            encoder_output = encoder_outputs[:, idx, :].unsqueeze(1)\n",
    "            #encoder_output = [src len, 1, enc hid dim * 2]\n",
    "            \n",
    "            mask = self.create_mask(src_tensor[:, idx].unsqueeze(1))\n",
    "            # print(\"mask shape: \", mask.shape)\n",
    "            \n",
    "            #mask = [1, src len]\n",
    "\n",
    "            # Start with the start of the sentence token\n",
    "            decoder_input = torch.LongTensor([SOS_IDX]).to(device)\n",
    "\n",
    "            # Number of sentence to generate\n",
    "            endnodes = []\n",
    "            number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "            # starting node -  hidden vector, previous node, word id, logp, length\n",
    "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "            nodes = PriorityQueue()\n",
    "\n",
    "            # start the queue\n",
    "            nodes.put((-node.eval(), node))\n",
    "            qsize = 1\n",
    "\n",
    "            # start beam search\n",
    "            while True:\n",
    "                # give up when decoding takes too long\n",
    "                if qsize > 2000: break\n",
    "\n",
    "                # fetch the best node\n",
    "                # score is log p divides by the length scaled by some constants\n",
    "                score, n = nodes.get()\n",
    "                \n",
    "                # wordid is simply the numercalized integer of the word\n",
    "                decoder_input = n.wordid\n",
    "                decoder_hidden = n.h\n",
    "\n",
    "                if n.wordid.item() == EOS_IDX and n.prevNode != None:\n",
    "                    endnodes.append((score, n))\n",
    "                    # if we reached maximum # of sentences required\n",
    "                    if len(endnodes) >= number_required:\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # decode for one step using decoder\n",
    "                # decoder_input = SOS_IDX\n",
    "                # decoder_hidden = [1, dec hid dim]\n",
    "                # encoder_outputs = [src len, 1, enc hid dim * 2]\n",
    "                # mask = [batch size, src len]\n",
    "                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, decoder_hidden, encoder_output, mask)\n",
    "\n",
    "                # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "                log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "                nextnodes = []  #the next possible node you can move to\n",
    "\n",
    "                # we only select beam_width amount of nextnodes\n",
    "                for new_k in range(beam_width):\n",
    "                    decoded_t = indexes[0][new_k].view(-1)\n",
    "                    log_p = log_prob[0][new_k].item()\n",
    "\n",
    "                    node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.len + 1)\n",
    "                    score = -node.eval()\n",
    "                    nextnodes.append((score, node))\n",
    "\n",
    "                # put them into queue\n",
    "                for i in range(len(nextnodes)):\n",
    "                    score, nn = nextnodes[i]\n",
    "                    nodes.put((score, nn))\n",
    "                    # increase qsize\n",
    "                qsize += len(nextnodes) - 1\n",
    "\n",
    "            # Once everything is finished, choose nbest paths, back trace them\n",
    "            if len(endnodes) == 0:\n",
    "                endnodes = [nodes.get() for _ in range(topk)]\n",
    "\n",
    "            utterances = []\n",
    "            for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "                utterance = []\n",
    "                utterance.append(n.wordid)\n",
    "                # back trace\n",
    "                while n.prevNode != None:\n",
    "                    n = n.prevNode\n",
    "                    utterance.append(n.wordid)\n",
    "\n",
    "                utterance = utterance[::-1]\n",
    "                utterances.append(utterance)\n",
    "\n",
    "            decoded_batch.append(utterances)\n",
    "\n",
    "        return decoded_batch\n",
    "    \n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        self.h = hiddenstate  #define the hidden state\n",
    "        self.prevNode = previousNode  #where does it come from\n",
    "        self.wordid = wordId  #the numericalized integer of the word\n",
    "        self.logp = logProb  #the log probability\n",
    "        self.len = length  #the current length\n",
    "\n",
    "    def eval(self, alpha=0.7):\n",
    "        # the score will be simply the log probability normalized by the length \n",
    "        # we add some small number to avoid division error\n",
    "        # read https://arxiv.org/abs/1808.10006 to understand how alpha is selected\n",
    "        return self.logp / float(self.len + 1e-6) ** (alpha)\n",
    "    \n",
    "    #this is the function for comparing between two beamsearchnodes, whether which one is better\n",
    "    #it is called when you called \"put\"\n",
    "    #the__  denotes that this function is inherent with anything that is of \"object\" type\n",
    "    def __lt__(self, other):\n",
    "        return self.len < other.len\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.len > other.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8015, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(6192, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=6192, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 21,172,528 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        #put them to GPU\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "\n",
    "        #clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make prediction\n",
    "        output = model(src, src_len, trg)\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        #remove the first output (SOS) and then reshape both output and trg so we can calculate loss\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        #calculate loss and update\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are calling our implemented beam-search function <code>decode</code>.  To show it, we print some sample so we can take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            #put them to GPU\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "            #src = [src len, batch size]\n",
    "            #trg = [src len, batch size]\n",
    "            #src_len = [batch size]\n",
    "\n",
    "            #turn off teacher forcing\n",
    "            #make prediction\n",
    "            output = model(src, src_len, trg, 0) \n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "            decoded_batch = model.decode(src, src_len, trg, method='beam-search')\n",
    "            decoded_batch_list.append(decoded_batch)\n",
    "            \n",
    "            #remove the first output (SOS) and then reshape both output and trg so we can calculate loss\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        #this is optional; you don't have to; printing first three examples\n",
    "        print(\"print samples from first decode batch\")\n",
    "        for sentence_index in decoded_batch_list[0][:3]:\n",
    "            decode_text_arr = [vocab_transform[TRG_LANGUAGE].lookup_token(i) for i in sentence_index[0]]\n",
    "            decode_sentence = \" \".join(decode_text_arr[1:-1])\n",
    "            print(\"pred target : {}\".format(decode_sentence))\n",
    "        \n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print samples from first decode batch\n",
      "pred target : A <unk> in a a in a a a a .\n",
      "pred target : A child in a blue and and a a a a a .\n",
      "pred target : A woman and a a a a a a a in a in in the background .\n",
      "Epoch: 01 | Time: 1m 21s\n",
      "\tTrain Loss: 4.658 | Train PPL: 105.477\n",
      "\t Val. Loss: 4.319 |  Val. PPL:  75.090\n",
      "print samples from first decode batch\n",
      "pred target : A young child is a red ball ball .\n",
      "pred target : A man is his hands while he is standing in the the of the ocean .\n",
      "pred target : Three large dogs are in the snow in the snow .\n",
      "Epoch: 02 | Time: 1m 19s\n",
      "\tTrain Loss: 3.425 | Train PPL:  30.726\n",
      "\t Val. Loss: 3.633 |  Val. PPL:  37.812\n",
      "print samples from first decode batch\n",
      "pred target : A group of <unk> walk down the street with their flags .\n",
      "pred target : Kids are riding in a amusement park .\n",
      "pred target : A brown dog jumps jumping with a yellow ball in its mouth .\n",
      "Epoch: 03 | Time: 1m 19s\n",
      "\tTrain Loss: 2.762 | Train PPL:  15.836\n",
      "\t Val. Loss: 3.378 |  Val. PPL:  29.318\n",
      "print samples from first decode batch\n",
      "pred target : Two Indian children in in formal clothing performing a a dance dance .\n",
      "pred target : A woman works on the floor at the porch .\n",
      "pred target : A little boy in a yellow t - shirt hangs on a van as he children by children .\n",
      "Epoch: 04 | Time: 1m 20s\n",
      "\tTrain Loss: 2.353 | Train PPL:  10.515\n",
      "\t Val. Loss: 3.341 |  Val. PPL:  28.256\n",
      "print samples from first decode batch\n",
      "pred target : Two kids are dancing at a <unk> .\n",
      "pred target : People and <unk> are in the desert .\n",
      "pred target : A little girl walks along a small stream .\n",
      "Epoch: 05 | Time: 1m 18s\n",
      "\tTrain Loss: 2.044 | Train PPL:   7.718\n",
      "\t Val. Loss: 3.257 |  Val. PPL:  25.964\n",
      "print samples from first decode batch\n",
      "pred target : A man in a crowded bar with his hand on his forehead .\n",
      "pred target : A <unk> player crouches on the cheek .\n",
      "pred target : A man in a green jacket smiles .\n",
      "Epoch: 06 | Time: 1m 20s\n",
      "\tTrain Loss: 1.812 | Train PPL:   6.120\n",
      "\t Val. Loss: 3.296 |  Val. PPL:  26.997\n",
      "print samples from first decode batch\n",
      "pred target : A baseball player with a red helmet and white pants is <unk> his bat with his bat while running runs .\n",
      "pred target : A man racer in a blue shirt , a a black shirt , down the down the lane .\n",
      "pred target : Two women are standing a a a , while talking over a .\n",
      "Epoch: 07 | Time: 1m 14s\n",
      "\tTrain Loss: 1.626 | Train PPL:   5.084\n",
      "\t Val. Loss: 3.340 |  Val. PPL:  28.206\n",
      "print samples from first decode batch\n",
      "pred target : A man is a baby in a highchair .\n",
      "pred target : A man walking up the <unk> of while wearing a a a .\n",
      "pred target : Two men from opposite opposite opposite direction are running a a ball .\n",
      "Epoch: 08 | Time: 1m 17s\n",
      "\tTrain Loss: 1.480 | Train PPL:   4.391\n",
      "\t Val. Loss: 3.387 |  Val. PPL:  29.579\n",
      "print samples from first decode batch\n",
      "pred target : A man is showing the drum and a little boy swings his his with his small drum .\n",
      "pred target : Man in a blue helmet is on a bicycle on traffic .\n",
      "pred target : The picture is a picture of a man and a woman at some sort of <unk> .\n",
      "Epoch: 09 | Time: 1m 17s\n",
      "\tTrain Loss: 1.356 | Train PPL:   3.881\n",
      "\t Val. Loss: 3.448 |  Val. PPL:  31.439\n",
      "print samples from first decode batch\n",
      "pred target : A single man in a black t - shirt is the other people in a party .\n",
      "pred target : A woman is taking a photo on her camera .\n",
      "pred target : A woman is <unk> food food at the park .\n",
      "Epoch: 10 | Time: 1m 13s\n",
      "\tTrain Loss: 1.252 | Train PPL:   3.496\n",
      "\t Val. Loss: 3.594 |  Val. PPL:  36.373\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print samples from first decode batch\n",
      "pred target : A man is scrapping some trees .\n",
      "pred target : Young man is on a skateboard , holding his cellphone .\n",
      "pred target : A man and boy boy on a rocky beach .\n",
      "| Test Loss: 3.214 | Test PPL:  24.868 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut5-model.pt'))\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Inference - Beam Search\n",
    "\n",
    "Here, in inference, we shall use beam search instead of argmax.  Instead of using the highest probability word right away, we simply call our <code>decode</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_beam_search(src_sentence, trg_sentence, model, device, max_len = 50):\n",
    "\n",
    "    model.eval()\n",
    "    src_tensor = text_transform[SRC_LANGUAGE](src_sentence)\n",
    "    # src_tensor = [src len]\n",
    "\n",
    "    trg_tensor = text_transform[SRC_LANGUAGE](trg_sentence)\n",
    "    # trg_tensor = [trg len]\n",
    "\n",
    "    src_len = torch.tensor([len(src_tensor)])\n",
    "    # src_len = [1]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(src_tensor).unsqueeze(1).to(device)\n",
    "    # src_tensor = [src len, 1]\n",
    "\n",
    "    trg_tensor = torch.LongTensor(trg_tensor).unsqueeze(1).to(device)\n",
    "    # trg_tensor = [trg len, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decoded_batch = model.decode(src_tensor, src_len, trg_tensor, method='beam-search')\n",
    "        \n",
    "    #there is only one sentence, so index 0\n",
    "    for i in decoded_batch[0]:\n",
    "        decode_text_arr = [vocab_transform[TRG_LANGUAGE].lookup_token(j) for j in i]\n",
    "                    \n",
    "    return decode_text_arr[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iter = Multi30k(split=('valid'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "sample = next(valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#de\n",
    "src_sentence = sample[0]\n",
    "src_sentence = src_sentence.rstrip(\"\\n\")\n",
    "src_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A group of men are loading cotton onto a truck'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#en\n",
    "trg_sentence = sample[1]\n",
    "trg_sentence = trg_sentence.rstrip(\"\\n\")\n",
    "trg_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['A', 'group', 'of', 'men', 'are', 'hanging', 'out', 'on', 'a', 'truck', '.']\n"
     ]
    }
   ],
   "source": [
    "translation = translate_sentence_beam_search(src_sentence, trg_sentence, model, device)\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU\n",
    "\n",
    "Here, we can use beam_search decoding to see whether our BLEU increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(iterator, model, device, max_len = 50):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "    \n",
    "    for src, trg in iterator:\n",
    "        \n",
    "        pred_trg = translate_sentence_beam_search(src, trg, model, device, max_len)\n",
    "        \n",
    "        #tokenize target sentence so it can be compared with pred_trgs\n",
    "        trg = token_transform[TRG_LANGUAGE](trg.rstrip(\"\\n\"))\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "        \n",
    "        # print(\"pred_trg: \", pred_trg)\n",
    "        # print(\"trg: \", trg)\n",
    "\n",
    "    return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a BLEU of around 31.11 which is better than our previous tutorial (30).  Note that BLEU score is very sensitive to beam width.  Read this https://arxiv.org/pdf/1808.10006.pdf for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 31.11\n"
     ]
    }
   ],
   "source": [
    "test_iter = Multi30k(split=('test'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))\n",
    "bleu_score = calculate_bleu(test_iter, model, device)\n",
    "\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next tutorials we will be trying non-recurrent version by using convolutional network.  It is quite competitive in terms of performance but can be run much faster because it is not sequential."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
